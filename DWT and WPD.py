# -*- coding: utf-8 -*-
"""DWT and WPD.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WtBnVAKj9SedG0qzg3XxMFY1Fce0Whbc
"""

# Commented out IPython magic to ensure Python compatibility.
# All required imports
""" run them all"""
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

import keras.backend as K

import scipy.io as sio
# descriptive statistics
import scipy as sp
import pywt
import scipy.stats as stats

from io import BytesIO #needed for plot
import seaborn as sns; sns.set()

from keras.models import Model
from keras.layers import LSTM, Dropout, Dense, Conv1D, ConvLSTM2D, Input, Bidirectional, TimeDistributed,GRU, Reshape
from keras.layers import MaxPooling1D, Flatten, BatchNormalization
from keras.utils import plot_model
from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, GlobalMaxPooling1D, Lambda, Permute, Concatenate, GlobalAveragePooling1D
from numpy import save
from numpy import load
import sklearn
from scipy import stats
from sklearn import metrics
from keras.optimizers import Adam, SGD, RMSprop
from keras.models import load_model
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.model_selection import KFold, StratifiedKFold
from sklearn.metrics import classification_report
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
import pickle

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Select the Runtime > "Change runtime type" menu to enable a GPU accelerator, ')
  print('and then re-execute this cell.')
else:
  print(gpu_info)

#import dataset

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

cd drive/

cd My\ Drive

cd Transportation\ Mode\ Recognition

cd Processed\ Datset

cd Hands/

ls

Extacted_features = load('DWT_user3.npy')

Extacted_features.shape

#dataX = load('Hand_User3_fullX.npy')
datay = load('Hand_User3_fully.npy')

datay.shape

def one_hot_target(y):
    
    y_one_hot = pd.get_dummies(y)
    
    return y_one_hot.values

dataX.shape

y_ = one_hot_target(datay)
#X = feature_scaling(dataX)
#X_ = dataX   # this is without feature scaling
X, X_test, y, y_test = train_test_split(Extacted_features, y_ , test_size=0.1, random_state=42, shuffle = True)

"""# Feature **Extraction**"""

Extracted_features

## Feature extraction using Discrete wavelet transform
signal_length  = dataX.shape[1]
num_rows = 48 # 48 features extracted using dwt
training_size = dataX.shape[0]
numcols = training_size

Extracted_Features_tr = np.ndarray(shape = (numcols, num_rows, 19), dtype = 'float32', order = 'F') # declaring an empty array

Extracted_Features_tr.shape

waveletname = 'db1'

""" This cell performs DWT for feature extraction on the segmented data"""
for j in range(19):
  print(j)   # To account for 19 different sensors obtained
  for i in range(numcols):
      coeff = pywt.wavedec(dataX[i, :, j], waveletname,
                            level=6)
      cA6, cD6, cD5, cD4, cD3, cD2, cD1 = coeff

      Extracted_Features_tr[i, 0, j] = sp.mean(abs(cD1[:]))
      Extracted_Features_tr[i, 1, j] = sp.mean(abs(cD2[:]))
      Extracted_Features_tr[i, 2, j] = sp.mean(abs(cD3[:]))
      Extracted_Features_tr[i, 3, j] = sp.mean(abs(cD4[:]))
      Extracted_Features_tr[i, 4, j] = sp.mean(abs(cD5[:]))
      Extracted_Features_tr[i, 5, j] = sp.mean(abs(cD6[:]))
      Extracted_Features_tr[i, 6, j] = sp.mean(abs(cA6[:]))

      Extracted_Features_tr[i, 7, j] = sp.std(cD1[:])
      Extracted_Features_tr[i, 8, j] = sp.std(cD2[:])
      Extracted_Features_tr[i, 9, j] = sp.std(cD3[:])
      Extracted_Features_tr[i, 10, j] = sp.std(cD4[:])
      Extracted_Features_tr[i, 11, j] = sp.std(cD5[:])
      Extracted_Features_tr[i, 12, j] = sp.std(cD6[:])
      Extracted_Features_tr[i, 13, j] = sp.std(cA6[:])

      Extracted_Features_tr[i, 14, j] = stats.skew(cD1[:])
      Extracted_Features_tr[i, 15, j] = stats.skew(cD2[:])
      Extracted_Features_tr[i, 16, j] = stats.skew(cD3[:])
      Extracted_Features_tr[i, 17, j] = stats.skew(cD4[:])
      Extracted_Features_tr[i, 18, j] = stats.skew(cD5[:])
      Extracted_Features_tr[i, 19, j] = stats.skew(cD6[:])
      Extracted_Features_tr[i, 20, j] = stats.skew(cA6[:])

      Extracted_Features_tr[i, 21, j] = stats.kurtosis(cD1[:])
      Extracted_Features_tr[i, 22, j] = stats.kurtosis(cD2[:])
      Extracted_Features_tr[i, 23, j] = stats.kurtosis(cD3[:])
      Extracted_Features_tr[i, 24, j] = stats.kurtosis(cD4[:])
      Extracted_Features_tr[i, 25, j] = stats.kurtosis(cD5[:])
      Extracted_Features_tr[i, 26, j] = stats.kurtosis(cD6[:])
      Extracted_Features_tr[i, 27, j] = stats.kurtosis(cA6[:])

      Extracted_Features_tr[i, 28, j] = sp.median(cD1[:])
      Extracted_Features_tr[i, 29, j] = sp.median(cD2[:])
      Extracted_Features_tr[i, 30, j] = sp.median(cD3[:])
      Extracted_Features_tr[i, 31, j] = sp.median(cD4[:])
      Extracted_Features_tr[i, 32, j] = sp.median(cD5[:])
      Extracted_Features_tr[i, 33, j] = sp.median(cD6[:])
      Extracted_Features_tr[i, 34, j] = sp.median(cA6[:])

      Extracted_Features_tr[i, 35, j] = np.sqrt(
          np.mean(cD1[:] ** 2)); 
      Extracted_Features_tr[i, 36, j] = np.sqrt(
          np.mean(cD2[:] ** 2));
      Extracted_Features_tr[i, 37, j] = np.sqrt(
          np.mean(cD3[:] ** 2));
      Extracted_Features_tr[i, 38, j] = np.sqrt(
          np.mean(cD4[:] ** 2));
      Extracted_Features_tr[i, 39, j] = np.sqrt(
          np.mean(cD5[:] ** 2));
      Extracted_Features_tr[i, 40, j] = np.sqrt(
          np.mean(cD6[:] ** 2));
      Extracted_Features_tr[i, 41, j] = np.sqrt(
          np.mean(cA6[:] ** 2));

      Extracted_Features_tr[i, 42, j] = sp.mean(
          abs(cD1[:])) / sp.mean(abs(cD2[:]))
      Extracted_Features_tr[i, 43, j] = sp.mean(
          abs(cD2[:])) / sp.mean(abs(cD3[:]))
      Extracted_Features_tr[i, 44, j] = sp.mean(
          abs(cD3[:])) / sp.mean(abs(cD4[:]))
      Extracted_Features_tr[i, 45, j] = sp.mean(
          abs(cD4[:])) / sp.mean(abs(cD5[:]))
      Extracted_Features_tr[i, 46, j] = sp.mean(
          abs(cD5[:])) / sp.mean(abs(cD6[:]))
      Extracted_Features_tr[i, 47, j] = sp.mean(
          abs(cD6[:])) / sp.mean(abs(cA6[:]))

db1 = pywt.Wavelet('db1')

""" This cell performs WPD for feature extraction on the segmented data"""

for j in range(19):                           
  print(j)   # To account for 19 different sensors obtained
  for i in range(numcols):
    wp= pywt.WaveletPacket(dataX[i,:,j], db1, mode='symmetric', maxlevel=6)

    Extracted_Features[i,0,j]=sp.mean(abs(wp['a'].data)) ##  Mean statistic calculated
    Extracted_Features[i,1,j]=sp.mean(abs(wp['aa'].data))
    Extracted_Features[i,2,j]=sp.mean(abs(wp['aaa'].data))
    Extracted_Features[i,3,j]=sp.mean(abs(wp['aaaa'].data))
    Extracted_Features[i,4,j]=sp.mean(abs(wp['aaaaa'].data))
    Extracted_Features[i,5,j]=sp.mean(abs(wp['aaaaaa'].data))
    Extracted_Features[i,6,j]=sp.mean(abs(wp['d'].data))
    Extracted_Features[i,7,j]=sp.mean(abs(wp['dd'].data))
    Extracted_Features[i,8,j]=sp.mean(abs(wp['ddd'].data))
    Extracted_Features[i,9,j]=sp.mean(abs(wp['dddd'].data))
    Extracted_Features[i,10,j]=sp.mean(abs(wp['ddddd'].data))
    Extracted_Features[i,11,j]=sp.mean(abs(wp['dddddd'].data))

    Extracted_Features[i,12,j]=sp.std(wp['a'].data)  ## Standard deviation calculated
    Extracted_Features[i,13,j]=sp.std(wp['aa'].data)
    Extracted_Features[i,14,j]=sp.std(wp['aaa'].data)
    Extracted_Features[i,15,j]=sp.std(wp['aaaa'].data)
    Extracted_Features[i,16,j]=sp.std(wp['aaaaa'].data)
    Extracted_Features[i,17,j]=sp.std(wp['aaaaaa'].data)
    Extracted_Features[i,18,j]=sp.std(wp['d'].data)
    Extracted_Features[i,19,j]=sp.std(wp['dd'].data)
    Extracted_Features[i,20,j]=sp.std(wp['ddd'].data)
    Extracted_Features[i,21,j]=sp.std(wp['dddd'].data)
    Extracted_Features[i,22,j]=sp.std(wp['ddddd'].data)
    Extracted_Features[i,23,j]=sp.std(wp['dddddd'].data)

    Extracted_Features[i,24,j]=sp.median(wp['a'].data)
    Extracted_Features[i,25,j]=sp.median(wp['aa'].data)
    Extracted_Features[i,26,j]=sp.median(wp['aaa'].data)
    Extracted_Features[i,27,j]=sp.median(wp['aaaa'].data)
    Extracted_Features[i,28,j]=sp.median(wp['aaaaa'].data)
    Extracted_Features[i,29,j]=sp.median(wp['aaaaaa'].data)
    Extracted_Features[i,30,j]=sp.median(wp['d'].data)
    Extracted_Features[i,31,j]=sp.median(wp['dd'].data)
    Extracted_Features[i,32,j]=sp.median(wp['ddd'].data)
    Extracted_Features[i,33,j]=sp.median(wp['dddd'].data)
    Extracted_Features[i,34,j]=sp.median(wp['ddddd'].data)
    Extracted_Features[i,35,j]=sp.median(wp['dddddd'].data)

    Extracted_Features[i,36,j]=stats.skew(wp['a'].data)
    Extracted_Features[i,37,j]=stats.skew(wp['aa'].data)
    Extracted_Features[i,38,j]=stats.skew(wp['aaa'].data)
    Extracted_Features[i,39,j]=stats.skew(wp['aaaa'].data)
    Extracted_Features[i,40,j]=stats.skew(wp['aaaaa'].data)
    Extracted_Features[i,41,j]=stats.skew(wp['aaaaaa'].data)
    Extracted_Features[i,42,j]=stats.skew(wp['d'].data)
    Extracted_Features[i,43,j]=stats.skew(wp['dd'].data)
    Extracted_Features[i,44,j]=stats.skew(wp['ddd'].data)
    Extracted_Features[i,45,j]=stats.skew(wp['dddd'].data)
    Extracted_Features[i,46,j]=stats.skew(wp['ddddd'].data)
    Extracted_Features[i,47,j]=stats.skew(wp['dddddd'].data)

    Extracted_Features[i,48,j]=stats.kurtosis(wp['a'].data)
    Extracted_Features[i,49,j]=stats.kurtosis(wp['aa'].data)
    Extracted_Features[i,50,j]=stats.kurtosis(wp['aaa'].data)
    Extracted_Features[i,51,j]=stats.kurtosis(wp['aaaa'].data)
    Extracted_Features[i,52,j]=stats.kurtosis(wp['aaaaa'].data)
    Extracted_Features[i,53,j]=stats.kurtosis(wp['aaaaaa'].data)
    Extracted_Features[i,54,j]=stats.kurtosis(wp['d'].data)
    Extracted_Features[i,55,j]=stats.kurtosis(wp['dd'].data)
    Extracted_Features[i,56,j]=stats.kurtosis(wp['ddd'].data)
    Extracted_Features[i,57,j]=stats.kurtosis(wp['dddd'].data)
    Extracted_Features[i,58,j]=stats.kurtosis(wp['ddddd'].data)
    Extracted_Features[i,59,j]=stats.kurtosis(wp['dddddd'].data)

    Extracted_Features[i,60,j]=np.sqrt(np.mean(wp['a'].data**2))   #RMS Value
    Extracted_Features[i,61,j]=np.sqrt(np.mean(wp['aa'].data**2))
    Extracted_Features[i,62,j]=np.sqrt(np.mean(wp['aaa'].data**2))
    Extracted_Features[i,63,j]=np.sqrt(np.mean(wp['aaaa'].data**2))
    Extracted_Features[i,64,j]=np.sqrt(np.mean(wp['aaaaa'].data**2))
    Extracted_Features[i,65,j]=np.sqrt(np.mean(wp['aaaaaa'].data**2))
    Extracted_Features[i,66,j]=np.sqrt(np.mean(wp['d'].data**2))
    Extracted_Features[i,67,j]=np.sqrt(np.mean(wp['dd'].data**2))
    Extracted_Features[i,68,j]=np.sqrt(np.mean(wp['ddd'].data**2))
    Extracted_Features[i,69,j]=np.sqrt(np.mean(wp['dddd'].data**2))
    Extracted_Features[i,70,j]=np.sqrt(np.mean(wp['ddddd'].data**2))
    Extracted_Features[i,71,j]=np.sqrt(np.mean(wp['dddddd'].data**2))

    Extracted_Features[i,72,j]=sp.mean(abs(wp['a'].data))/sp.mean(abs(wp['aa'].data))
    Extracted_Features[i,73,j]=sp.mean(abs(wp['aa'].data))/sp.mean(abs(wp['aaa'].data))
    Extracted_Features[i,74,j]=sp.mean(abs(wp['aaa'].data))/sp.mean(abs(wp['aaaa'].data))
    Extracted_Features[i,75,j]=sp.mean(abs(wp['aaaa'].data))/sp.mean(abs(wp['aaaaa'].data))
    Extracted_Features[i,76,j]=sp.mean(abs(wp['aaaaa'].data))/sp.mean(abs(wp['aaaaaa'].data))
    Extracted_Features[i,77,j]=sp.mean(abs(wp['aaaaaa'].data))/sp.mean(abs(wp['d'].data))
    Extracted_Features[i,78,j]=sp.mean(abs(wp['d'].data))/sp.mean(abs(wp['dd'].data))
    Extracted_Features[i,79,j]=sp.mean(abs(wp['dd'].data))/sp.mean(abs(wp['ddd'].data))
    Extracted_Features[i,80,j]=sp.mean(abs(wp['ddd'].data))/sp.mean(abs(wp['dddd'].data))
    Extracted_Features[i,81,j]=sp.mean(abs(wp['dddd'].data))/sp.mean(abs(wp['ddddd'].data))
    Extracted_Features[i,82,j]=sp.mean(abs(wp['ddddd'].data))/sp.mean(abs(wp['dddddd'].data))

# Building model
Extacted_features = Extracted_Features_tr.reshape(-1,48,19,1)

Extacted_features.shape

save('DWT_user3',Extracted_Features_tr)

ls

"""# Deep Learning **Analysis**"""

""" Conv2D model archtecture
  The model just treats it like a 2d image and pulls out features with filters 
  followed by maxpooling

"""

input_ = Input(shape = (48, 19, 1)) # Model decent accuracy

x = Conv2D(filters = 64, kernel_size = (4,2), activation = 'relu', padding = 'same')(input_)
x = Conv2D(filters = 64, kernel_size = (4,2), activation = 'relu', padding = 'same')(x)
x = BatchNormalization()(x)
x = MaxPooling2D(pool_size = (2,2))(x)

x = Conv2D(filters = 128, kernel_size = (4,2), activation = 'relu', padding = 'same')(x)
x = BatchNormalization()(x)
x = MaxPooling2D(pool_size = (2,1))(x) 
#x = Dropout(0.1)(x)

x = Conv2D(filters = 256, kernel_size = (3,3), activation = 'relu', padding = 'same')(x)
x = BatchNormalization()(x)
x = MaxPooling2D(pool_size = (2,2))(x)

x = Conv2D(filters = 256, kernel_size = (3,3), activation = 'relu')(x)
x = BatchNormalization()(x)
x = MaxPooling2D(pool_size = (2,2))(x)

x = Flatten()(x)
x = Dense(200, activation = 'relu')(x)
output = Dense(8, activation = 'softmax')(x)

""" BILSTM double flip model

This model at first performs a Bidirectional LSTM on the 2D data. Then it transposes the data, i.e. shifts
it by 90 degrees and then runs a Bidirectional LSTM on this, then it concatenates the pooled results and 
a simple dense layer is then used
"""


input_ = Input(shape = (48,19,))              # Best performer upto now
rnn1 = Bidirectional(LSTM(200, return_sequences=True))
x1 = rnn1(input_)
x1 = LSTM(400, return_sequences=True)(x1)
#x1 = LSTM(200, return_sequences= True)(x1)
x1 = GlobalMaxPooling1D()(x1)

rnn2 = Bidirectional(LSTM(200, return_sequences=True))
permutor = Lambda(lambda t: K.permute_dimensions(t, pattern=(0, 2, 1)))
x2 = permutor(input_)
x2 = rnn2(x2)
x2 = LSTM(400, return_sequences=True)(x2)
#x2 = LSTM(200, return_sequences= True)(x2)
x2 = GlobalMaxPooling1D()(x2)
x = Concatenate(axis=1)([x1,x2])
x = Dense(400, activation = 'relu')(x)
#x = Dense(300, activation = 'relu')(x)
x = Dense(200, activation = 'relu')(x)
x = Dense(100, activation = 'relu')(x)
output = Dense(8, activation = 'softmax')(x)

""" This model is the normal neural network that is of 7 layers, many different 
    architectures where experimented before finalising this one"""

input_ = Input(shape = (48,19,))  # Fixed model at 150 epochs
x = Flatten()(input_)
#x = Dense(400, activation = 'relu')(x)
x = Dense(400, activation = 'relu')(x)
#x = Dropout(0.1)(x)
x = Dense(400, activation = 'relu')(x)
x = Dense(400, activation = 'relu')(x)
x = Dropout(0.25)(x)
x = Dense(400, activation = 'relu')(x)
##x = Dense(400, activation = 'relu')(x)
x = Dense(300, activation = 'relu')(x)
x = Dropout(0.2)(x)
x = Dense(200, activation = 'relu')(x)
#x = Dropout(0.3)(x)
x = Dense(100, activation = 'relu')(x)
output = Dense(8, activation = 'softmax')(x)

model = Model(input_, output)

model = Model(input_, output)

model.summary()

model.compile(optimizer='adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])

X_ = X.reshape(62569, 48, 19,1)

X_.shape

r = model.fit(X_, y, batch_size = 512, epochs = 20, validation_split = 0.1)

pred_test   = model.predict(X_test_)
max_y_pred_test = np.round(pred_test)
print_performance_metrics()

X_test_ = X_test.reshape(6953, 48, 19,1)

save('Extracted_Features_DWT_user3', Extracted_Features_tr)

def print_performance_metrics():
    print('Accuracy:', np.round(metrics.accuracy_score(y_test, max_y_pred_test),4))
    print('Precision:', np.round(metrics.precision_score(y_test, 
                                max_y_pred_test,average='weighted'),4))
    print('Recall:', np.round(metrics.recall_score(y_test, max_y_pred_test,
                                               average='weighted'),4))
    print('F1 Score:', np.round(metrics.f1_score(y_test, max_y_pred_test,
                                               average='weighted'),4))
    print('Cohen Kappa Score:', np.round(metrics.cohen_kappa_score(y_test.argmax(axis=1), max_y_pred_test.argmax(axis=1)),4))
    print('Matthews Corrcoef:', np.round(metrics.matthews_corrcoef(y_test.argmax(axis=1), max_y_pred_test.argmax(axis=1)),4)) 
    print('ROC AUC:', (metrics.roc_auc_score(y_test, max_y_pred_test,average='macro')))
    print('\t\tClassification Report:\n', metrics.classification_report(y_test, max_y_pred_test))





"""# Machine Learning **Analysis**"""

from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.metrics import accuracy_score, log_loss
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.svm import SVC

from sklearn.utils.testing import all_estimators   # all_estimators is a class the contains a lot of estimators

Extracted_features_ML = Extacted_features.reshape(-1, 48*19)

#y_ = one_hot_target(datay)
#X = feature_scaling(dataX)
#X_ = dataX   # this is without feature scaling
X, X_test, y, y_test = train_test_split(Extracted_features_ML, datay , test_size=0.1, random_state=42, shuffle = True)

# Feature scaling using minmax scaler
scaler = MinMaxScaler()
scaler.fit_transform(X)
scaler.fit(X_test)

X.shape

# Feature scaling using Standard scaler
scaler = StandardScaler()
scaler.fit_transform(X)
scaler.fit(X_test)

from xgboost import XGBClassifier
print(xgboost.__version__)

model = XGBClassifier()
model.fit(X,y)
acc = accuracy_score(y_test, model.predict(X_test))

acc

from lightgbm import LGBMClassifier

model = LGBMClassifier()
model.fit(X,y)
acc = accuracy_score(y_test, model.predict(X_test))

acc

pip install catboost

from catboost import CatBoostClassifier

model = CatBoostClassifier()
model.fit(X,y)
acc = accuracy_score(y_test, model.predict(X_test))

acc

np.argmax(np.array(model.feature_importances_))

""" In this section we experiment with a variety of classifiers to decide upon 
  the best classifier"""

all_estimators_ = all_estimators() # instntiating the object
i = 0
desired_estimators = []
for name, class_ in all_estimators_:
  if ~hasattr(class_, 'predict_proba'):
    desired_estimators.append(class_)
    print(i," ", class_, " ", name)
    i += 1

# MLP, Gradient Boosting, Extratrees

trial_index_list = [6,23,55,66,92,106,112,136,152,149,162,182,192]

trial_classifiers_list = []
for i in trial_index_list:
  trial_classifiers_list.append(all_estimators_[i][1])

trial_classifiers_list # These were the models that were tested with the dataset

i = 1
for class_ in trial_classifiers_list:
  model_class =class_
  #i = 1
  try:
    print(' getting on..........', model_class)
    model = model_class()
    model.fit(X, y)
    acc = accuracy_score(y_test, model.predict(X_test))
    print(i, " ", model_class, acc)
    i = i+1
  except:
      print('sorry failed on :', model)
  #i+=1

""" In this section we perform grid search for optimum hyperparameter selection"""

seed=1
models = ['EXT','RF']
          
            
clfs = [
        ExtraTreesClassifier(random_state = seed),
        
        #GradientBoostingClassifier(random_state=seed),
        RandomForestClassifier(random_state=seed,n_jobs=-1)
        #KNeighborsClassifier(n_jobs=-1),
        #SVC(random_state=seed,probability=True),
        #LogisticRegression(solver='newton-cg', multi_class='multinomial')
        ]

params = {
            models[0]:{ 'n_estimators':[150,100,50,200],
                       'min_samples_split': [2,3],'min_samples_leaf': [1,2]},
            
            models[1]:{'n_estimators':[150,50,100,200], 'criterion':['gini'],
                       'min_samples_split':[2,3,4],
                      'min_samples_leaf': [4,3]},
           # models[3]:{'n_neighbors':[5,10,4,20,12,15,7,8,9,17], 'weights':['distance'],'leaf_size':[15,10,20,5,12,7]},
            #models[4]: {'C':[100,50,150], 'tol': [0.005, 0.001,0.01, 0.007,0.015],
             #          'kernel':['sigmoid']},
            #models[5]: {'C':[2000,1000,1500,3000], 'tol': [0.0001]}
         }

test_scores = []

for name, estimator in zip(models,clfs):
    print(name)
    clf = GridSearchCV(estimator, params[name], scoring='balanced_accuracy',
                       refit='True', n_jobs=-1,cv = 5)
    clf.fit(X,y)
    print("best params: " + str(clf.best_params_))
    print("best scores: " + str(clf.best_score_))
    acc = accuracy_score(y_test, clf.predict(X_test))
    test_scores.append((name,acc,clf.best_score_))

test_scores

rf  = ExtraTreesClassifier(n_estimators= 215)

rf.fit(X,y)

acc = accuracy_score(y_test, rf.predict(X_test))

acc

