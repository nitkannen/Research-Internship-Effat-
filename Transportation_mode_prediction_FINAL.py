# -*- coding: utf-8 -*-
"""Transportation Mode prediction2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FWopTujm4q0eLpaaK3K6GHvTPAAbRsW6
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

from io import BytesIO #needed for plot
import seaborn as sns; sns.set()

from keras.models import Model
from keras.layers import LSTM, Dropout, Dense, Conv1D, ConvLSTM2D, Input, Bidirectional, TimeDistributed,GRU
from keras.layers import MaxPooling1D, Flatten, BatchNormalization
from keras.utils import plot_model
from keras.models import load_model
from numpy import save
from numpy import load
import sklearn
from scipy import stats
from sklearn import metrics
from keras.optimizers import Adam, SGD, RMSprop
from keras.models import load_model
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.model_selection import KFold, StratifiedKFold
from sklearn.metrics import classification_report
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
import pickle

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Select the Runtime > "Change runtime type" menu to enable a GPU accelerator, ')
  print('and then re-execute this cell.')
else:
  print(gpu_info)

from keras.layers import Conv2D, MaxPooling2D, BatchNormalization

input_ = Input(shape = (48, 19, 1))

x = Conv2D(filters = 64, kernel_size = (4,2), activation = 'relu', padding = 'same')(input_)
x = Conv2D(filters = 64, kernel_size = (4,2), activation = 'relu', padding = 'same')(x)
x = BatchNormalization()(x)
x = MaxPooling2D(pool_size = (2,2))(x)

x = Conv2D(filters = 128, kernel_size = (4,2), activation = 'relu', padding = 'same')(x)
x = BatchNormalization()(x)
x = MaxPooling2D(pool_size = (2,1))(x) 

x = Conv2D(filters = 256, kernel_size = (3,3), activation = 'relu', padding = 'same')(x)
x = BatchNormalization()(x)
x = MaxPooling2D(pool_size = (2,2))(x)

x = Conv2D(filters = 256, kernel_size = (3,3), activation = 'relu')(x)
x = BatchNormalization()(x)
x = MaxPooling2D(pool_size = (2,2))(x)

x = Flatten()(x)
x = Dense(200, activation = 'relu')(x)
output = Dense(8, activation = 'softmax')(x)

model = Model(input_, output)

model.summary()

"""# Read Data"""

def read_data():  # returns a merged data file of features and target labels
    #The features are input
    
    df = pd.read_csv('Hips_Motion.txt', delimiter = ' ',dtype = 'float32')
    df.columns = ['Timestep','AccX','AccY','AccZ','Gyroscope_X','Gyroscope_Y','Gyroscope_Z','Mag_meter_X','Mag_meter_Y','Mag_meter_Z','Orientation_w','Orientation_x','Orientation_y','Orientation_z','Gravity_X','Gravity_Y','Gravity_Z','Linear_accX','Linear_accY','Linear_accZ','Pressure','Altitude','Temperature']
    
    d = pd.read_csv('Hips_Motion.txt', delimiter = ' ',dtype  = str,usecols=[0])
    d.columns = ['Timestep']
    df['Timestep'] = d['Timestep']
    df['Timestep']= df['Timestep'].apply(lambda x : x.split('.')[0])
    #return df
    #df.head()
    
    # input target labels
    
    dfLabel = pd.read_csv('Label.txt', delimiter=' ', dtype = 'float64')
    dfLabel.columns = ['Timestep','Coarse_label','Fine_label','Road','Traffic','Tunnel','Social','Food']
    
    dLabel = pd.read_csv('Label.txt', delimiter=' ', dtype = str, usecols=[0])
    dLabel.columns = ['Timestep']
    #dLabel.columns = ['Timestep','Coarse_label','Fine_label','Road','Traffic','Tunnel','Social','Food']
    
    dfLabel['Timestep'] = dLabel['Timestep']
    #dfLabel.head()
    #len(dfLabel)
    
    x = df
    y = dfLabel[['Timestep','Coarse_label','Fine_label']]
    
    data = pd.merge(x, y, on = 'Timestep')
    
    return data.dropna()

#data = read_data()

#data.head()

"""# Data Preprocessing"""

def filter_data(data):
    
    data_filtered = data[(data['Coarse_label'] > 0  )].reset_index(drop = True) # removing claas labels zero
    
    #print(data_filtered['Coarse_label'].value_counts())
    
    return data_filtered

# Hyperparameters

LEN_OF_SEQUENCE = 200
OVERLAP = 50 # 25% overlap
FEATURE_DIMENSION = 19

def make_numpy_matrix(data) :
    # This function converts the data into a matrix that could be fed inside lstm. Sequence len = 200
    
    X = []
    y = []
    for i in range(OVERLAP,len(data) - LEN_OF_SEQUENCE ,LEN_OF_SEQUENCE):
            #print(i)
        #label = 1.0

        accX = data['AccX'].values[i:i + LEN_OF_SEQUENCE]
        accX1 = data['AccX'].values[i - 50:i + LEN_OF_SEQUENCE- 50]
        accY = data['AccY'].values[i:i + LEN_OF_SEQUENCE]
        accY1 = data['AccY'].values[i - 50:i + LEN_OF_SEQUENCE- 50]
        accZ = data['AccZ'].values[i:i + LEN_OF_SEQUENCE]
        accZ1 = data['AccZ'].values[i - 50:i + LEN_OF_SEQUENCE- 50]

        Gyro_X = data['Gyroscope_X'].values[i :i + LEN_OF_SEQUENCE]
        Gyro_X1 = data['Gyroscope_X'].values[i - 50:i + LEN_OF_SEQUENCE- 50]
        Gyro_Y = data['Gyroscope_Y'].values[i :i + LEN_OF_SEQUENCE]
        Gyro_Y1 = data['Gyroscope_Y'].values[i - 50:i + LEN_OF_SEQUENCE- 50]
        Gyro_Z = data['Gyroscope_Z'].values[i :i + LEN_OF_SEQUENCE]
        Gyro_Z1 = data['Gyroscope_Z'].values[i - 50:i + LEN_OF_SEQUENCE- 50]

        Mag_X = data['Mag_meter_X'].values[i :i + LEN_OF_SEQUENCE]
        Mag_X1 = data['Mag_meter_X'].values[i - 50:i + LEN_OF_SEQUENCE- 50]
        Mag_Y = data['Mag_meter_Y'].values[i :i + LEN_OF_SEQUENCE]
        Mag_Y1 = data['Mag_meter_Y'].values[i - 50:i + LEN_OF_SEQUENCE- 50]
        Mag_Z = data['Mag_meter_Z'].values[i :i + LEN_OF_SEQUENCE]
        Mag_Z1 = data['Mag_meter_Z'].values[i - 50:i + LEN_OF_SEQUENCE- 50]

        Linear_X = data['Linear_accX'].values[i :i + LEN_OF_SEQUENCE]
        Linear_X1 = data['Linear_accX'].values[i - 50:i + LEN_OF_SEQUENCE- 50]
        Linear_Y = data['Linear_accY'].values[i :i + LEN_OF_SEQUENCE]
        Linear_Y1 = data['Linear_accY'].values[i - 50:i + LEN_OF_SEQUENCE- 50]
        Linear_Z = data['Linear_accZ'].values[i :i + LEN_OF_SEQUENCE]
        Linear_Z1 = data['Linear_accZ'].values[i - 50:i + LEN_OF_SEQUENCE- 50]

        Gravity_X = data['Gravity_X'].values[i :i + LEN_OF_SEQUENCE]
        Gravity_X1 = data['Gravity_X'].values[i - 50:i + LEN_OF_SEQUENCE- 50]
        Gravity_Y = data['Gravity_Y'].values[i :i + LEN_OF_SEQUENCE]
        Gravity_Y1 = data['Gravity_Y'].values[i - 50:i + LEN_OF_SEQUENCE- 50]
        Gravity_Z = data['Gravity_Z'].values[i :i + LEN_OF_SEQUENCE]
        Gravity_Z1 = data['Gravity_Z'].values[i - 50:i + LEN_OF_SEQUENCE- 50]

        Orient_X = data['Orientation_x'].values[i :i + LEN_OF_SEQUENCE]
        Orient_X1 = data['Orientation_x'].values[i - 50:i + LEN_OF_SEQUENCE- 50]
        Orient_W = data['Orientation_w'].values[i :i + LEN_OF_SEQUENCE]
        Orient_W1 = data['Orientation_w'].values[i - 50:i + LEN_OF_SEQUENCE- 50]
        Orient_Y = data['Orientation_y'].values[i :i + LEN_OF_SEQUENCE]
        Orient_Y1 = data['Orientation_y'].values[i - 50:i + LEN_OF_SEQUENCE- 50]
        Orient_Z = data['Orientation_z'].values[i :i + LEN_OF_SEQUENCE]
        Orient_Z1 = data['Orientation_z'].values[i - 50:i + LEN_OF_SEQUENCE- 50]

        X.append([accX,accY,accZ,Gyro_X,Gyro_Y,Gyro_Z,Mag_X,Mag_Y, Mag_Z,
                  Linear_X, Linear_Y, Linear_Z,Gravity_X, Gravity_Y, Gravity_Z
                 ,Orient_X, Orient_Y, Orient_Z,Orient_W])

        X.append([accX1,accY1,accZ1,Gyro_X1,Gyro_Y1,Gyro_Z1,Mag_X1,Mag_Y1, Mag_Z1,
                   Linear_X1, Linear_Y1, Linear_Z1,Gravity_X1, Gravity_Y1, Gravity_Z1
                 ,Orient_X1, Orient_Y1, Orient_Z1,Orient_W1])

        #Here we are taking the mode of labels in the time sequence instead of the last recorder activity

        labels = stats.mode(data['Coarse_label'][i :i + LEN_OF_SEQUENCE])
        y.append(labels[0][0])

        labels = stats.mode(data['Coarse_label'][i - 50:i + LEN_OF_SEQUENCE- 50])
        y.append(labels[0][0])

        #y.append(label)
        #y.append(label)
    X = np.asarray(X,dtype = 'float32')
    X  = X.reshape((-1,200,19))

    y = np.asarray(y, dtype = 'float32')
    
    return X,y

data = read_data()
filtered_data = filter_data(data)
X,y = make_numpy_matrix(filtered_data)

df = pd.DataFrame(y, columns = ['label'])

df['label'].value_counts() ## Check distribution of data

"""## Save the processed data into .npy file"""

save('Hips_5_7_y',y)

save('Hips_5_7_X', X)

pwd

cd C:\\Users\\Nithish\\Desktop\\Research Internship
cd SHLDataset_User1Hips_v1/
cd release/
cd User1/
cd 050717/

"""#Load** Data"""

from google.colab import drive
drive.mount('/content/drive', force_remount= True)

cd drive/

cd My\ Drive

cd Transportation\ Mode\ Recognition

cd Processed\ Datset

cd Bag/

cd Torso

cd ..

ls

Hips_model = load_model('CNN2- full')

model = Hips_model

def concatenate(X,y, new_dataX, new_datay): # cocatenates the old matrix to the new one

  dataX = np.concatenate((X, new_dataX), axis = 0)
  datay = np.concatenate((y, new_datay), axis = 0)
  #print(data.shape)

  return dataX, datay

dataX = load('Bag_full_dataX.npy')
datay = load('Bag_full_datay.npy')

dataX = load('Torso_User1_fullX.npy')
datay = load('Torso_User1_fully.npy')
X  = load('Torso_User2_fullX.npy')
y = load('Torso_User2_fully.npy')
dataX, datay = concatenate(dataX, datay,X,y )

X = load('Torso_User3_fullX.npy')
y = load('Torso_User3_fully.npy')
dataX, datay = concatenate(dataX, datay,X,y )



dataX_ = load('2_full_data_X.npy')
datay_ = load('2_full_data_y.npy')
X  = load('full_data_X.npy')
y = load('full_data_y.npy')
dataX_, datay_ = concatenate(dataX_, datay_,X,y )
X = load('3_full_data_X.npy')
y = load('3_full_data_y.npy')
dataX_, datay_ = concatenate(dataX_, datay_,X,y )

dataX, datay = dataX_, datay_

dataX, datay = concatenate(dataX, datay,dataX_,datay_)

dataX = load('Hands_full_dataX.npy')
datay = load('Hands_full_datay.npy')

dataX, datay = concatenate(dataX, datay,X,y)

datay.shape

# This Loads the saved data and concatenates it to forma a large dataset

dataX = load('Hips_5_7_X.npy')
datay = load('Hips_5_7_y.npy')
X = load('Hips_2_3_X.npy')
y = load('Hips_2_3_y.npy')
dataX, datay = concatenate(dataX, datay,X,y )

X = load('Hips_2_6_X.npy')
y = load('Hips_2_6_y.npy')
dataX, datay = concatenate(dataX, datay,X,y )

X = load('Hips_3_6_X.npy')
y = load('Hips_3_6_y.npy')
dataX, datay = concatenate(dataX, datay,X,y )

X = load('Hips_3_7_X.npy')
y = load('Hips_3_7_y.npy')
dataX, datay = concatenate(dataX, datay,X,y )

X = load('Hips_4_5_X.npy')
y = load('Hips_4_5_y.npy')
dataX, datay = concatenate(dataX, datay,X,y )

X = load('Hips_5_7_X.npy')
y = load('Hips_5_7_y.npy')
dataX, datay = concatenate(dataX, datay,X,y )

X = load('Hips_6_3_X.npy')
y = load('Hips_6_3_y.npy')
dataX, datay = concatenate(dataX, datay,X,y )

X = load('preview_22_X.npy')
y = load('preview_22_y.npy')
dataX, datay = concatenate(dataX, datay,X,y)

X = load('preview_26_X.npy')
y = load('preview_26_y.npy')
dataX, datay = concatenate(dataX, datay,X,y)

save('full_data_X' ,dataX)
save('full_data_y.npy', datay)

#np.random.seed(32)
def shuffle_along_axis(a, axis): # This is for shuffling the dataset
    
    np.random.seed(32)
    
    idx = np.random.rand(*a.shape,).argsort(axis=axis)
    
    return np.take_along_axis(a,idx,axis=axis)

def feature_scaling(X):
    # Converting the 3d matrix to 2d for feature scaling
    X_2d = X.reshape((-1,200*19))
    
    scaler = MinMaxScaler(feature_range=(0, 1))
    
    X_scaled = scaler.fit_transform(X_2d)
    
    # Rescaling the 2d matrix to 3d
    X_scaled_3d = X_scaled.reshape((-1,200,19))
    
    print(X_scaled_3d.shape)
    
    return(X_scaled_3d)

def one_hot_target(y):
    
    y_one_hot = pd.get_dummies(y)
    
    return y_one_hot.values

dataX.shape

y_ = one_hot_target(datay)
#X = feature_scaling(dataX)
X_ = dataX   # this is without feature scaling
X, X_test, y, y_test = train_test_split(X_, y_ , test_size=0.1, random_state=42, shuffle = True)

dataX.shape

datay.shape

y.shape

X_test.shape

X.shape

"""# Model"""

#Hyperparameters used

#EPOCHS = 12-20
#BATCH_SIZE = 512
# CONV LSTM MODEL
#n_steps = 4
#n_Length = 50
#LOSS  = CATEGORICAL CROSS ENTROPY
#OPTIMIZER = ADAM lr = 0.01
#HIDDEN UNITS1 = 200
#HIDEEN UNITS2 = 100
#DROPOUT BETWEEN 0.2 AND 0.3 dual

train_X = X.reshape(X.shape[0], 4, 50,19)
#test_X = X_test.reshape(X_test.shape[0], 4, 50,19)

train_X.shape

train_X_ = X.reshape(X.shape[0], 4,1, 50,19)
#test_X_ = X_test.reshape(X_test.shape[0], 4,1, 50,19)

train_X_.shape

test_X = X_test.reshape(X_test.shape[0], 4,50,19)

X.shape

def LSTM_Model()  : 
    print(y.sum(0))
    input_ = Input(shape = (200,19))
    x = LSTM(200, return_sequences=True)(input_)
    #x = Dropout(0.1)(x)
    x = (LSTM(200, return_sequences= True))(x)
    x = LSTM(200)(x)
    x = Dropout(0.2)(x)
    #x = Dropout(0.2)(x)
    x = Dense(units = 200, activation='relu')(x)
    output = Dense(8,  activation='softmax')(x)

    LSTM_model = Model(input_, output)
    return LSTM_model

m = LSTM_Model()

model,r = train_model_()

def stacked_GRU():

  input_ = Input(shape = (200,19))
  x = GRU(200, return_sequences =True)(input_)
  x = GRU(200, return_sequences = True)(x)
  x = Dropout(0.2)(x)
  x = GRU(200)(x)
  x = Dense(100, activation = 'relu')(x)
  output = Dense(8, activation = 'softmax')(x)

  GRU_model = Model(input_, output)

  return GRU_model

def train_model_():
  model = stacked_GRU()
  model.compile(loss = 'categorical_crossentropy', optimizer = Adam(), metrics = ['accuracy'])
  print(model.summary())
  r = model.fit(X_train, y_train, batch_size = 512, epochs = 16,validation_data =  (X_test,y_test))
  #return 4, 6
  return model,r

"""# CNN2"""

def CNN2():

  input_ = Input(shape = (4,50,19))
  x = TimeDistributed(Conv1D(filters = 64, kernel_size = 3, activation = 'relu'))(input_)
  x = TimeDistributed(Conv1D(filters =64,kernel_size =  3,activation = 'relu'))(x)
  x = TimeDistributed(Dropout(0.1))(x)
  x = TimeDistributed(MaxPooling1D(pool_size=2))(x)
  x = TimeDistributed(Conv1D(filters =128, kernel_size = 3,activation = 'relu'))(x)
  x = TimeDistributed(Conv1D(filters =256,kernel_size = 4,activation ='relu'))(x)
  x = TimeDistributed(Dropout(0.2))(x)
  x = TimeDistributed(MaxPooling1D(pool_size=2))(x)
  x = (Flatten())(x)
  x = Dense(200, activation = 'relu')(x)
  output = Dense(8, activation = 'softmax')(x)

  model = Model(input_, output)
  
  return model

model = CNN2()

model.summary()

model.compile(loss = 'categorical_crossentropy', optimizer = Adam(), metrics = ['accuracy'])

#model = load_model('full')



r = model.fit(train_X, y, batch_size = 512, epochs = 16, validation_data = (test_X,y_test))

model.save('Torso')

print("Train Accuracy:",np.mean(train_scores), "ConvLTSM3- User 1 tOrso")
print("Test Accuracy:", np.mean(scores), "ConvLSTM3 - uSER 1 tORSO ")

scores

train_X.shape

Kfold_CV = KFold(n_splits = 10, random_state=42)
for train_index, test_index in Kfold_CV.split(train_X):
  print(train_index)
  print('\n')

"""## DRIVER"""

def KFoldCV(train_X,y):  # This performs 10 Fold Cross validation on user dependant model

    scores = []
    train_scores = []
    Kfold_CV = KFold(n_splits = 10, random_state=42)
    i = 0

    for train_index, test_index in Kfold_CV.split(train_X):

        #CNN2()
        i += 1
        print(i)
        if i>9 and i<11:
          #break
          Conv_LSTM_model  = stacked_GRU()
          Conv_LSTM_model.compile(loss = 'categorical_crossentropy', optimizer = Adam(), metrics = ['accuracy'])
          if i<10:
            print(Conv_LSTM_model.summary())

          r = Conv_LSTM_model.fit(train_X[train_index], y[train_index], batch_size = 512, epochs = 16, validation_data = (train_X[test_index],y[test_index]))

          # Evaluate on train
          pred_train = Conv_LSTM_model.predict(train_X[train_index])
          pred_train_te = pred_train.argmax(axis = 1)
          train_te = y[train_index].argmax(axis = 1)
          accuracy_train   = accuracy_score(train_te, pred_train_te)
          train_scores.append(accuracy_train)
          
          # Evaluate on test
          pred  = Conv_LSTM_model.predict(train_X[test_index])
          pred_te = pred.argmax(axis = 1)
          test_te = y[test_index].argmax(axis = 1)
          accuracy = accuracy_score(test_te, pred_te)
          scores.append(accuracy)

    return scores, train_scores, Conv_LSTM_model,r

scores, train_scores, model, r = KFoldCV(X,y)

model.save("GRU - full")

np.mean(scores)

print("Train Accuracy:",np.mean(train_scores))
print("Test Accuracy:", np.mean(scores))



pred_test   = model.predict(test_X)
max_y_pred_test = np.round(pred_test)
print_performance_metrics()
#print_confusion_matrix_and_save()

def print_performance_metrics():
    print('Accuracy:', np.round(metrics.accuracy_score(y_test, max_y_pred_test),4))
    print('Precision:', np.round(metrics.precision_score(y_test, 
                                max_y_pred_test,average='weighted'),4))
    print('Recall:', np.round(metrics.recall_score(y_test, max_y_pred_test,
                                               average='weighted'),4))
    print('F1 Score:', np.round(metrics.f1_score(y_test, max_y_pred_test,
                                               average='weighted'),4))
    print('Cohen Kappa Score:', np.round(metrics.cohen_kappa_score(y_test.argmax(axis=1), max_y_pred_test.argmax(axis=1)),4))
    print('Matthews Corrcoef:', np.round(metrics.matthews_corrcoef(y_test.argmax(axis=1), max_y_pred_test.argmax(axis=1)),4)) 
    print('ROC AUC:', (metrics.roc_auc_score(y_test, max_y_pred_test,average='macro')))
    print('\t\tClassification Report:\n', metrics.classification_report(y_test, max_y_pred_test))

def print_confusion_matrix_and_save():
    mat = confusion_matrix(y_test.argmax(axis=1), max_y_pred_test.argmax(axis=1))
    sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False)
    plt.title('Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    #plt.show()

    plt.savefig("Hand.jpg")
    # Save SVG in a fake file object.
    f = BytesIO()
    plt.savefig(f, format="svg")

"""CONV LSTM Model"""

train_X = X_train.reshape(X_train.shape[0], 4, 50,19)
test_X = X_test.reshape(X_test.shape[0], 4, 50,19)

train_X.shape

test_X.shape

def train_model():
  model = CNN()
  model.compile(loss = 'categorical_crossentropy', optimizer = Adam(), metrics = ['accuracy'])
  print(model.summary())
  r = model.fit(train_X, y_train, batch_size = 512, epochs = 16,validation_data =  (test_X,y_test))
  #return 4, 6
  return model,r

def plot_history(r):
    plt.figure(figsize = (8,5))
    plt.title('CONV_Bidirectional_LSTM_Model', color = 'white', fontsize = 20)
    plt.plot(r.history['accuracy'], label = 'Train Accuracy')
    plt.xlabel('Epochs', color = 'white', fontsize = 20)
    #plt.xticklabels()
    plt.ylabel('Accuracy', color = 'white', fontsize = 20)
    plt.plot(r.history['val_accuracy'], label = 'Test Accuracy')
    plt.legend()

"""# CONV LSTM 1"""

# CONV LSTM model1
def initialise_model_CONV_LSTM_model1():

    input_ = Input(shape = (4,50,19))
    x = TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu'))(input_)
    x = TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu'))(x)
    x = TimeDistributed(Conv1D(filters=128, kernel_size=3, activation='relu'))(x)
    x = TimeDistributed(Dropout(0.2))(x)
    x = TimeDistributed(MaxPooling1D(pool_size=2))(x)
    x = TimeDistributed(Flatten())(x)
    x = LSTM(100, return_sequences= True)(x)
    x =LSTM(200, return_sequences=True)(x)
    x = LSTM(200)(x)
    x = Dropout(0.2)(x)
    x = Dense(200, activation='relu')(x)
    output = Dense(8, activation='softmax')(x)

    Conv_LSTM_model = Model(input_, output)

    return Conv_LSTM_model

model,r = train_model()

model.save('Conv_LSTM1 full')

"""## CONV LSTM 3"""

# CONV LSTM model3
def initialise_model_CONV_LSTM_model3():

    input_ = Input(shape = (4,50,19))
    x = TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu'))(input_)
    x = TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu'))(x)
    x = TimeDistributed(Conv1D(filters=128, kernel_size=3, activation='relu'))(x)
    x = TimeDistributed(Dropout(0.2))(x)
    x = TimeDistributed(MaxPooling1D(pool_size=2))(x)
    x = TimeDistributed(Flatten())(x)
    x = Bidirectional(LSTM(100, return_sequences= True))(x)
    x = Bidirectional(LSTM(200, return_sequences=True))(x)
    x = LSTM(400)(x)
    x = Dropout(0.2)(x)
    x = Dense(200, activation='relu')(x)
    output = Dense(8, activation='softmax')(x)

    Conv_LSTM_model = Model(input_, output)

    return Conv_LSTM_model

model,r = train_model()

model_evaluate(model, test_X)

model.save('Conv_LSTM_model2')

plot_history(r)

print("The mean test accuracy is:",np.mean(np.array(scores)),"- Conv_LSTM 1")

print("The mean training accuracy is: ",np.mean(np.array(train_scores)),"- Conv_LSTM 1")

model, r = train_model()

"""ConvLSTM2D"""

train_X_.shape

#ConvLSTM2D Model1
def initialise_Conv_LSTM2D_model1():
    input_ = Input(shape = ((4,1,50,19)))
    x = ConvLSTM2D(filters = 64, kernel_size = (1,3), activation = 'relu')(input_)
    #x = (Conv1D(filters=64, kernel_size=3, activation='relu'))(x)
    x = TimeDistributed(Conv1D(filters=128, kernel_size=3, activation='relu'))(x)
    x = TimeDistributed(Dropout(0.2))(x)
    x = TimeDistributed(MaxPooling1D(pool_size=2))(x)
    x = TimeDistributed(Flatten())(x)
    x = Bidirectional(LSTM(100, return_sequences= True))(x)
    x = LSTM(200)(x)
    x = Dropout(0.2)(x)
    x = Dense(200, activation='relu')(x)
    output = Dense(8, activation='softmax')(x)
    
    
    ConvLSTM2D_model = Model(input_, output)

    return ConvLSTM2D_model

#ConvLSTM2D Model3
def initialise_Conv_LSTM2D_model3():

    input_ = Input(shape = ((4,1,50,19)))
    x = ConvLSTM2D(filters = 128, kernel_size = (1,3), activation = 'relu', return_sequences= True)(input_)
    #x = ConvLSTM2D(filters = 64, kernel_size = (1,3), activation = 'relu')(x)
    #x = ConvLSTM2D(filters = 128, kernel_size = (1,3), activation = 'relu')(x)
    x = TimeDistributed(Dropout(0.2))(x)
    x = TimeDistributed(Flatten())(x)
    x = Bidirectional(LSTM(100, return_sequences= True))(x)
    x =LSTM(200, return_sequences=True)(x)
    x = LSTM(200)(x)
    x = Dropout(0.2)(x)
    x = Dense(200, activation='relu')(x)
    output = Dense(8, activation='softmax')(x)

    ConvLSTM2D_model = Model(input_, output)

    return ConvLSTM2D_model

# ConvLSTM2D model2
def initialise_Conv_LSTM2D_model2():
    input_ = Input(shape = ((4,1,50,19)))
    x = ConvLSTM2D(filters = 64, kernel_size = (1,3), activation = 'relu')(input_)
    x = TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu'))(x)
    x = TimeDistributed(Conv1D(filters=128, kernel_size=3, activation='relu'))(x)
    x = TimeDistributed(Dropout(0.2))(x)
    x = TimeDistributed(MaxPooling1D(pool_size=2))(x)
    x = TimeDistributed(Flatten())(x)
    x  = LSTM(100)(x)
    x = Dropout(0.2)(x)
    x = Dense(200, activation='relu')(x)
    output = Dense(8, activation='softmax')(x)
    
    ConvLSTM2D_model = Model(input_, output)

    return ConvLSTM2D_model



"""# CONV LSTM 2D **4**"""

# ConvLSTM2D model4
def initialise_Conv_LSTM2D_model4():
    input_ = Input(shape = ((4,1,50,19)))
    x = ConvLSTM2D(filters = 64, kernel_size = (1,3), activation = 'relu', return_sequences= True)(input_)
    x = ConvLSTM2D(filters = 128, kernel_size = (1,3), activation = 'relu')(x)
    x = TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu'))(x)
    x = TimeDistributed(Conv1D(filters=128, kernel_size=3, activation='relu'))(x)
    x = TimeDistributed(Dropout(0.2))(x)
    x = TimeDistributed(MaxPooling1D(pool_size=2))(x)
    x = TimeDistributed(Flatten())(x)
    x = Bidirectional(LSTM(100, return_sequences= True))(x)
    x  = LSTM(200)(x)
    x = Dropout(0.2)(x)
    x = Dense(200, activation='relu')(x)
    output = Dense(8, activation='softmax')(x)
    
    ConvLSTM2D_model = Model(input_, output)

    return ConvLSTM2D_model

def stacked_CONVLSTM2D(): 

    input_ = Input(shape = (4,1,50,19,))
    x = ConvLSTM2D(filters = 64, kernel_size = (1,3), activation = 'relu', return_sequences = True)(input_)
    x = (BatchNormalization())(x)
    x = ConvLSTM2D(filters =  64, kernel_size = (1,3), activation = 'relu', return_sequences= True)(x)
    x = (BatchNormalization())(x)
    x = TimeDistributed(Dropout(0.1))(x)
    x = ConvLSTM2D(filters = 128, kernel_size = (1,4), activation = 'relu', return_sequences= True)(x)
    x = (BatchNormalization())(x)
    x = ConvLSTM2D(filters  = 128, kernel_size = (1,4), activation = 'relu')(x)
    #x = TimeDistributed(Conv1D(filters = 64, kernel_size= 3, activation = 'relu'))(x)
    #x = TimeDistributed(Conv1D(filters = 128, kernel_size = 4, activation='relu'))(x)
    x = TimeDistributed(Dropout(0.2))(x)
    #x = TimeDistributed(MaxPooling1D(pool_size=2))(x)
    x = Flatten()(x)
    x = Dense(200, activation='relu')(x)
    output = Dense(8, activation='softmax')(x)

    model = Model(input_, output)

    return model

def stacked_CONVLSTM2D_2(): 
  
  
    input_ = Input(shape = (4,1,50,19,))
    #x = ConvLSTM2D(filters = 64, kernel_size = (1,3), activation = 'relu', return_sequences = True)(input_)
    #x = (BatchNormalization())(input_)
    x = ConvLSTM2D(filters  = 64, kernel_size = (1,3), activation = 'relu')(input_)
    x = TimeDistributed(Conv1D(filters=128, kernel_size=3, activation='relu'))(x)
    x = TimeDistributed(Conv1D(filters=128, kernel_size=3, activation='relu'))(x)
    x = TimeDistributed(Dropout(0.2))(x)
    x = TimeDistributed(MaxPooling1D(pool_size=2))(x)
    x = TimeDistributed(Flatten())(x)
    x = TimeDistributed(Dropout(0.2))(x)
    x = Flatten()(x)
    x = Dense(200, activation='relu')(x)
    output = Dense(8, activation='softmax')(x)

    model = Model(input_, output)

    return model

def train_model1():
  model = initialise_Conv_LSTM2D_model2()
  model.compile(loss = 'categorical_crossentropy', optimizer = Adam(), metrics = ['accuracy'])
  print(model.summary())
  r = model.fit(train_X_ , y_train, batch_size = 512, epochs = 16,validation_data =  ( test_X_ ,y_test ))
  #return 4, 6
  return model,r

model,r  = train_model1()

model,r  = train_model()

def model_evaluate(model, test_X):
    pred = model.predict(test_X)
    y_te = y_test.argmax(axis = 1)
    pred_te = pred.argmax(axis = 1)
    print("CLASSIFICATION REPORT \n")
    print(classification_report(y_te, pred_te))
    print('\n \n')
    print('CONFUSION MATRIX \n')
    print(confusion_matrix(y_te, pred_te))

